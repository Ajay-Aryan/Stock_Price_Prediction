{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 18260.3438 - val_loss: 16378.0830\n",
      "Epoch 2/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15962.0059 - val_loss: 7280.6816\n",
      "Epoch 3/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4471.7417 - val_loss: 123.3282\n",
      "Epoch 4/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.1877 - val_loss: 102.8508\n",
      "Epoch 5/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 96.0602 - val_loss: 81.0161\n",
      "Epoch 6/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.4096 - val_loss: 62.7154\n",
      "Epoch 7/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.8156 - val_loss: 45.1042\n",
      "Epoch 8/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44.2027 - val_loss: 29.1380\n",
      "Epoch 9/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.1972 - val_loss: 16.1122\n",
      "Epoch 10/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.6666 - val_loss: 9.5303\n",
      "Epoch 11/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8551 - val_loss: 6.6190\n",
      "Epoch 12/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8924 - val_loss: 5.2602\n",
      "Epoch 13/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0679 - val_loss: 4.3893\n",
      "Epoch 14/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7186 - val_loss: 4.3591\n",
      "Epoch 15/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8450 - val_loss: 3.1090\n",
      "Epoch 16/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3027 - val_loss: 2.7193\n",
      "Epoch 17/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8709 - val_loss: 2.6230\n",
      "Epoch 18/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3850 - val_loss: 2.3373\n",
      "Epoch 19/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8315 - val_loss: 2.2948\n",
      "Epoch 20/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6195 - val_loss: 1.7534\n",
      "Epoch 21/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1146 - val_loss: 1.6467\n",
      "Epoch 22/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8396 - val_loss: 1.5573\n",
      "Epoch 23/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9297 - val_loss: 1.6487\n",
      "Epoch 24/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6270 - val_loss: 1.5650\n",
      "Epoch 25/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4252 - val_loss: 1.6343\n",
      "Epoch 26/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6412 - val_loss: 1.5498\n",
      "Epoch 27/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5988 - val_loss: 1.4639\n",
      "Epoch 28/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7150 - val_loss: 1.6987\n",
      "Epoch 29/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4345 - val_loss: 1.6217\n",
      "Epoch 30/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3825 - val_loss: 1.4910\n",
      "Epoch 31/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6133 - val_loss: 1.5980\n",
      "Epoch 32/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6250 - val_loss: 1.4644\n",
      "Epoch 33/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7103 - val_loss: 1.4836\n",
      "Epoch 34/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6197 - val_loss: 1.4614\n",
      "Epoch 35/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4509 - val_loss: 1.4734\n",
      "Epoch 36/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7377 - val_loss: 1.4876\n",
      "Epoch 37/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7148 - val_loss: 1.5569\n",
      "Epoch 38/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6578 - val_loss: 1.4930\n",
      "Epoch 39/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6054 - val_loss: 1.4552\n",
      "Epoch 40/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9331 - val_loss: 1.6721\n",
      "Epoch 41/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6454 - val_loss: 1.5400\n",
      "Epoch 42/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4222 - val_loss: 1.6824\n",
      "Epoch 43/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7631 - val_loss: 1.5759\n",
      "Epoch 44/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6657 - val_loss: 1.5111\n",
      "Epoch 45/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8593 - val_loss: 1.4449\n",
      "Epoch 46/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5820 - val_loss: 1.8702\n",
      "Epoch 47/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7688 - val_loss: 1.5206\n",
      "Epoch 48/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6281 - val_loss: 1.4384\n",
      "Epoch 49/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3706 - val_loss: 1.5587\n",
      "Epoch 50/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6459 - val_loss: 1.5369\n",
      "Epoch 51/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7644 - val_loss: 1.4298\n",
      "Epoch 52/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8594 - val_loss: 1.4300\n",
      "Epoch 53/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3247 - val_loss: 1.5332\n",
      "Epoch 54/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5112 - val_loss: 1.4653\n",
      "Epoch 55/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6192 - val_loss: 1.4311\n",
      "Epoch 56/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8922 - val_loss: 1.5332\n",
      "Epoch 57/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6622 - val_loss: 1.4517\n",
      "Epoch 58/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5378 - val_loss: 1.4447\n",
      "Epoch 59/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5952 - val_loss: 1.4015\n",
      "Epoch 60/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3822 - val_loss: 1.3960\n",
      "Epoch 61/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7752 - val_loss: 1.3934\n",
      "Epoch 62/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4692 - val_loss: 1.8542\n",
      "Epoch 63/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6117 - val_loss: 1.4875\n",
      "Epoch 64/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7338 - val_loss: 1.4254\n",
      "Epoch 65/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2822 - val_loss: 1.4494\n",
      "Epoch 66/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4555 - val_loss: 2.3233\n",
      "Epoch 67/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7095 - val_loss: 1.9339\n",
      "Epoch 68/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6940 - val_loss: 1.3798\n",
      "Epoch 69/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6154 - val_loss: 1.3742\n",
      "Epoch 70/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5785 - val_loss: 2.3608\n",
      "Epoch 71/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1080 - val_loss: 1.5745\n",
      "Epoch 72/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6107 - val_loss: 1.3680\n",
      "Epoch 73/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7364 - val_loss: 1.4343\n",
      "Epoch 74/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4564 - val_loss: 1.3569\n",
      "Epoch 75/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8714 - val_loss: 1.3534\n",
      "Epoch 76/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3474 - val_loss: 1.9349\n",
      "Epoch 77/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3928 - val_loss: 1.5851\n",
      "Epoch 78/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8512 - val_loss: 1.3915\n",
      "Epoch 79/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3549 - val_loss: 1.5348\n",
      "Epoch 80/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9548 - val_loss: 1.4608\n",
      "Epoch 81/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5333 - val_loss: 1.4331\n",
      "Epoch 82/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6971 - val_loss: 1.7476\n",
      "Epoch 83/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5990 - val_loss: 1.5024\n",
      "Epoch 84/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5297 - val_loss: 1.3496\n",
      "Epoch 85/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7202 - val_loss: 1.3640\n",
      "Epoch 86/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6558 - val_loss: 1.5405\n",
      "Epoch 87/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4048 - val_loss: 1.3209\n",
      "Epoch 88/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6529 - val_loss: 1.4270\n",
      "Epoch 89/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4797 - val_loss: 3.0354\n",
      "Epoch 90/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0614 - val_loss: 1.3136\n",
      "Epoch 91/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7157 - val_loss: 1.5007\n",
      "Epoch 92/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8765 - val_loss: 1.4719\n",
      "Epoch 93/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7681 - val_loss: 1.4959\n",
      "Epoch 94/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4346 - val_loss: 1.3104\n",
      "Epoch 95/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4271 - val_loss: 1.3975\n",
      "Epoch 96/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4143 - val_loss: 1.3216\n",
      "Epoch 97/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5336 - val_loss: 1.4890\n",
      "Epoch 98/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5625 - val_loss: 2.1454\n",
      "Epoch 99/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8543 - val_loss: 1.4677\n",
      "Epoch 100/100\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4920 - val_loss: 1.3856\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3871 \n",
      "Mean Squared Error on validation set: 1.3855767250061035\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Predictions on validation set:\n",
      "[[ 76.39784 ]\n",
      " [ 81.30567 ]\n",
      " [145.21924 ]\n",
      " [161.60161 ]\n",
      " [148.22942 ]\n",
      " [118.34009 ]\n",
      " [ 89.11    ]\n",
      " [ 95.206215]\n",
      " [ 99.36108 ]\n",
      " [182.00888 ]\n",
      " [198.31024 ]\n",
      " [157.60321 ]\n",
      " [124.465645]\n",
      " [155.13464 ]\n",
      " [ 99.7149  ]\n",
      " [163.3925  ]\n",
      " [116.30781 ]\n",
      " [161.87598 ]\n",
      " [163.47845 ]\n",
      " [124.74646 ]\n",
      " [ 83.98249 ]\n",
      " [ 80.43148 ]\n",
      " [142.34673 ]\n",
      " [ 93.42527 ]\n",
      " [153.96649 ]\n",
      " [152.44351 ]\n",
      " [160.16571 ]\n",
      " [160.57565 ]\n",
      " [160.89809 ]\n",
      " [150.24649 ]\n",
      " [145.1405  ]\n",
      " [159.95303 ]\n",
      " [ 78.73469 ]\n",
      " [ 83.24915 ]\n",
      " [110.50239 ]\n",
      " [157.50348 ]\n",
      " [159.95169 ]\n",
      " [177.34297 ]\n",
      " [146.15228 ]\n",
      " [ 84.756584]\n",
      " [195.02736 ]\n",
      " [159.01479 ]\n",
      " [162.21033 ]\n",
      " [161.39941 ]\n",
      " [ 96.055466]\n",
      " [ 81.2273  ]\n",
      " [145.14847 ]\n",
      " [ 82.248695]\n",
      " [141.15842 ]\n",
      " [149.39859 ]\n",
      " [ 99.62598 ]\n",
      " [150.43665 ]\n",
      " [121.60107 ]\n",
      " [144.70518 ]\n",
      " [ 82.63996 ]\n",
      " [162.20607 ]\n",
      " [152.28847 ]\n",
      " [ 80.4562  ]\n",
      " [130.61125 ]\n",
      " [174.76288 ]\n",
      " [158.15463 ]\n",
      " [101.43853 ]\n",
      " [ 80.87112 ]\n",
      " [ 81.04488 ]\n",
      " [164.3003  ]\n",
      " [127.61729 ]\n",
      " [107.54938 ]\n",
      " [200.03879 ]\n",
      " [126.16553 ]\n",
      " [159.42549 ]\n",
      " [ 97.020874]\n",
      " [106.56648 ]\n",
      " [ 92.618965]\n",
      " [117.207375]\n",
      " [175.57347 ]\n",
      " [163.32767 ]\n",
      " [147.10092 ]\n",
      " [155.97928 ]\n",
      " [145.534   ]\n",
      " [132.39413 ]\n",
      " [ 82.47958 ]\n",
      " [ 76.49463 ]\n",
      " [107.72154 ]\n",
      " [ 94.49455 ]\n",
      " [148.94926 ]\n",
      " [123.91571 ]\n",
      " [125.97552 ]\n",
      " [ 82.35956 ]\n",
      " [ 99.477196]\n",
      " [161.80266 ]\n",
      " [129.40063 ]\n",
      " [147.49263 ]\n",
      " [141.23373 ]\n",
      " [148.98671 ]\n",
      " [147.26617 ]\n",
      " [132.91739 ]\n",
      " [145.97928 ]\n",
      " [ 81.4675  ]\n",
      " [145.01329 ]\n",
      " [158.19788 ]\n",
      " [ 82.46827 ]\n",
      " [ 97.94169 ]\n",
      " [119.656586]\n",
      " [ 83.016335]\n",
      " [115.40837 ]\n",
      " [154.76334 ]\n",
      " [100.320786]\n",
      " [134.14668 ]\n",
      " [ 83.3853  ]\n",
      " [ 99.980995]\n",
      " [ 99.302635]\n",
      " [145.17503 ]\n",
      " [ 90.48178 ]\n",
      " [150.52148 ]\n",
      " [146.21376 ]\n",
      " [148.06613 ]\n",
      " [144.87126 ]\n",
      " [145.29115 ]\n",
      " [169.26477 ]\n",
      " [148.54796 ]\n",
      " [ 99.28355 ]\n",
      " [113.08861 ]\n",
      " [136.42746 ]\n",
      " [151.61327 ]\n",
      " [139.57033 ]\n",
      " [150.97942 ]\n",
      " [127.01484 ]\n",
      " [ 76.85786 ]\n",
      " [198.80643 ]\n",
      " [ 80.678955]\n",
      " [ 83.32338 ]\n",
      " [146.0379  ]\n",
      " [124.55518 ]\n",
      " [ 78.39422 ]\n",
      " [ 96.499825]\n",
      " [ 80.2143  ]\n",
      " [151.26086 ]\n",
      " [191.41568 ]\n",
      " [ 94.91384 ]\n",
      " [100.227905]\n",
      " [143.3446  ]\n",
      " [175.43237 ]\n",
      " [169.2631  ]\n",
      " [155.56686 ]\n",
      " [115.69591 ]\n",
      " [145.69778 ]\n",
      " [132.19926 ]\n",
      " [158.7883  ]\n",
      " [173.5664  ]\n",
      " [157.75497 ]\n",
      " [122.39155 ]\n",
      " [140.00574 ]\n",
      " [117.52666 ]\n",
      " [111.43606 ]\n",
      " [127.79804 ]\n",
      " [ 85.344345]\n",
      " [ 82.11504 ]\n",
      " [ 94.08371 ]\n",
      " [150.65475 ]\n",
      " [153.43695 ]\n",
      " [145.42741 ]\n",
      " [172.9286  ]\n",
      " [ 77.14549 ]\n",
      " [ 98.21613 ]\n",
      " [ 81.15265 ]\n",
      " [174.74817 ]\n",
      " [138.11494 ]\n",
      " [146.85172 ]\n",
      " [102.275215]\n",
      " [124.94737 ]\n",
      " [130.15941 ]\n",
      " [152.90265 ]\n",
      " [171.93286 ]\n",
      " [ 79.71265 ]\n",
      " [128.62447 ]\n",
      " [ 98.74709 ]\n",
      " [163.2174  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "path_to_dataset = \"C:/Users/Ajay/Downloads/archive (12)/individual_stocks_5yr/individual_stocks_5yr/AAP_data.csv\"\n",
    "df = pd.read_csv(path_to_dataset)\n",
    "\n",
    "# Preprocess the data\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.drop(columns=['Name', 'date'])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop(columns=['close'])\n",
    "y = df['close']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training (70%) and testing (30%) sets, but we will only use the training set for now\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss = model.evaluate(X_val, y_val)\n",
    "print(f'Mean Squared Error on validation set: {val_loss}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(X_val)\n",
    "print(\"Predictions on validation set:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
